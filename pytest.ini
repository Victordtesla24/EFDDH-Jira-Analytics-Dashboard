[pytest]
# Test file patterns
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test markers for prioritization
markers =
    priority_high: High priority tests (dashboard functionality and operations)
    priority_medium: Medium priority tests (metrics and visualizations)
    priority_low: Low priority tests (other functionality)
    dashboard: Dashboard functionality tests
    metrics: Metrics calculation tests
    visuals: Visualization tests
    deployment: Deployment and configuration tests
    integration: Integration tests
    unit: Unit tests

# Test directories
testpaths =
    src/tests/ui/dashboard
    src/tests/deployment
    src/tests/pages
    src/tests/components
    src/tests/utils
    src/tests/data
    src/tests/integration

# Configure test running
addopts =
    # Show detailed test progress
    -v
    # Show local variables in tracebacks
    --showlocals
    # Show 10 slowest tests
    --durations=10
    # Generate coverage reports
    --cov=src
    --cov-report=term-missing
    --cov-report=html
    # Run all tests regardless of markers
    # -m "priority_high or priority_medium or priority_low"
    # Fail if coverage is below threshold
    --cov-fail-under=80
    # Generate JUnit report
    --junitxml=test-results.xml

# Configure test coverage
[coverage:run]
source = src
branch = True
omit =
    */tests/*
    */__init__.py
    */migrations/*
    */settings.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise NotImplementedError
    if __name__ == .__main__.:
    pass
    raise ImportError

# Configure test categories
[test-categories]
dashboard =
    test_sprint_selector.py
    test_metrics.py
    test_sprint_progress.py
    test_issue_analysis.py
    test_file_handler.py

metrics =
    test_metrics.py
    test_calculations.py
    test_analytics.py

visuals =
    test_sprint_progress.py
    test_issue_analysis.py
    test_visualizations.py

deployment =
    test_streamlit_deployment.py
    test_config.py
    test_setup.py

# Configure logging during tests
[log_cli]
level = INFO
format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)
date_format = %Y-%m-%d %H:%M:%S

# Configure test warnings
[pytest-warnings]
filterwarnings =
    ignore::DeprecationWarning
    ignore::UserWarning
    error::RuntimeWarning

# Configure test timeouts
[pytest-timeout]
timeout = 300

# Configure parallel testing
[pytest-xdist]
numprocesses = auto
